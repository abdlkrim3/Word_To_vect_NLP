{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Modèle CSkip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.687449,
     "end_time": "2022-06-10T00:07:43.628962",
     "exception": false,
     "start_time": "2022-06-10T00:07:42.941513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028492,
     "end_time": "2022-06-10T00:07:43.976955",
     "exception": false,
     "start_time": "2022-06-10T00:07:43.948463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "papermill": {
     "duration": 0.03905,
     "end_time": "2022-06-10T00:07:44.046005",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.006955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = \"\"\"الذكاء الاصطناعي هو مجال يهتم بتطوير الأنظمة التي يمكنها أداء المهام التي تتطلب عادةً الذكاء البشري.\n",
    "تتضمن هذه المهام مثل التعرف على الصور، معالجة اللغة الطبيعية، والتنبؤ. يعمل الذكاء الاصطناعي على تحسين الكفاءة وجودة العمل في مختلف الصناعات.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02946,
     "end_time": "2022-06-10T00:07:44.105065",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.075605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "papermill": {
     "duration": 0.047874,
     "end_time": "2022-06-10T00:07:44.189429",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.141555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "sentences = re.sub('[^\\u0621-\\u064A\\s]', ' ', sentences) \n",
    "\n",
    "custom_stop_words = [\n",
    "    \"هذا\", \"في\", \"على\", \"هو\", \"هي\", \"من\", \"ما\", \"إلى\", \"أن\", \"عن\", \"و\", \"لا\"\n",
    "]\n",
    "words = word_tokenize(sentences)\n",
    "filtered_words = [word for word in words if word not in custom_stop_words]\n",
    "# remove 1 letter words\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "sentences = \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'الذكاء الاصطناعي مجال يهتم بتطوير الأنظمة التي يمكنها أداء المهام التي تتطلب عادة الذكاء البشري تتضمن هذه المهام مثل التعرف الصور معالجة اللغة الطبيعية والتنبؤ يعمل الذكاء الاصطناعي تحسين الكفاءة وجودة العمل مختلف الصناعات'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'الذكاء الاصطناعي مجال يهتم بتطوير الأنظمة التي يمكنها أداء المهام التي تتطلب عادة الذكاء البشري تتضمن هذه المهام مثل التعرف الصور معالجة اللغة الطبيعية والتنبؤ يعمل الذكاء الاصطناعي تحسين الكفاءة وجودة العمل مختلف الصناعات'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03198,
     "end_time": "2022-06-10T00:07:44.252318",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.220338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "papermill": {
     "duration": 0.042019,
     "end_time": "2022-06-10T00:07:44.325533",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.283514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = sentences.split()\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['الذكاء', 'الاصطناعي', 'هو', 'مجال', 'يهتم', 'بتطوير', 'الأنظمة', 'التي', 'يمكنها', 'أداء', 'المهام', 'التي', 'تتطلب', 'عادةً', 'الذكاء', 'البشري', '.', 'تتضمن', 'هذه', 'المهام', 'مثل', 'التعرف', 'على', 'الصور،', 'معالجة', 'اللغة', 'الطبيعية،', 'والتنبؤ', '.', 'يعمل', 'الذكاء', 'الاصطناعي', 'على', 'تحسين', 'الكفاءة', 'وجودة', 'العمل', 'في', 'مختلف', 'الصناعات', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokens = ' '.join(vocab)\n",
    "vocab = word_tokenize(sentences)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028574,
     "end_time": "2022-06-10T00:07:44.509784",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.481210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "papermill": {
     "duration": 0.042076,
     "end_time": "2022-06-10T00:07:44.582821",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.540745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_to_word = {i:x for (i, x) in enumerate(vocab)}\n",
    "word_to_id = {x:i for (i, x) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030502,
     "end_time": "2022-06-10T00:07:44.655784",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.625282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Skip-Gram Paires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "papermill": {
     "duration": 0.041799,
     "end_time": "2022-06-10T00:07:44.728375",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.686576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_training_data(vocab, word_to_id, window_size):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(len(vocab), i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[vocab[i]])\n",
    "            Y.append(word_to_id[vocab[j]])\n",
    "            \n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02839,
     "end_time": "2022-06-10T00:07:44.785437",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.757047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "papermill": {
     "duration": 0.037456,
     "end_time": "2022-06-10T00:07:44.851567",
     "exception": false,
     "start_time": "2022-06-10T00:07:44.814111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_dims(x, y):\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    y = np.expand_dims(y, axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_training_data(vocab, word_to_id, 3)\n",
    "x, y = expand_dims(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "          3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  6,  6,\n",
       "          6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,\n",
       "          9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
       "         11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14,\n",
       "         14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16,\n",
       "         17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19,\n",
       "         19, 19, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 22, 22,\n",
       "         22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24,\n",
       "         25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 27, 27, 27, 27, 28,\n",
       "         28, 28]]),\n",
       " array([[ 1,  2,  3,  0,  2,  3,  4,  0,  1,  3,  4,  5,  0,  1,  2,  4,\n",
       "          5,  6,  1,  2,  3,  5,  6,  7,  2,  3,  4,  6,  7,  8,  3,  4,\n",
       "          5,  7,  8,  9,  4,  5,  6,  8,  9, 10,  5,  6,  7,  9, 10, 11,\n",
       "          6,  7,  8, 10, 11, 12,  7,  8,  9, 11, 12, 13,  8,  9, 10, 12,\n",
       "         13, 14,  9, 10, 11, 13, 14, 15, 10, 11, 12, 14, 15, 16, 11, 12,\n",
       "         13, 15, 16, 17, 12, 13, 14, 16, 17, 18, 13, 14, 15, 17, 18, 19,\n",
       "         14, 15, 16, 18, 19, 20, 15, 16, 17, 19, 20, 21, 16, 17, 18, 20,\n",
       "         21, 22, 17, 18, 19, 21, 22, 23, 18, 19, 20, 22, 23, 24, 19, 20,\n",
       "         21, 23, 24, 25, 20, 21, 22, 24, 25, 26, 21, 22, 23, 25, 26, 27,\n",
       "         22, 23, 24, 26, 27, 28, 23, 24, 25, 27, 28, 24, 25, 26, 28, 25,\n",
       "         26, 27]]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generated training data\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028474,
     "end_time": "2022-06-10T00:07:45.302438",
     "exception": false,
     "start_time": "2022-06-10T00:07:45.273964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "papermill": {
     "duration": 0.038991,
     "end_time": "2022-06-10T00:07:45.370607",
     "exception": false,
     "start_time": "2022-06-10T00:07:45.331616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_parameters(vocab_size, emb_size):\n",
    "    wrd_emb = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    w = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    return wrd_emb, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.divide(np.exp(z), np.sum(np.exp(z), axis=0, keepdims=True) + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inds, params):\n",
    "    wrd_emb, w = params\n",
    "    word_vec = wrd_emb[inds.flatten(), :].T\n",
    "    z = np.dot(w, word_vec)\n",
    "    out = softmax(z)\n",
    "    \n",
    "    cache = inds, word_vec, w, z\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_hat):\n",
    "    m = y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(y_hat * np.log(y + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029027,
     "end_time": "2022-06-10T00:07:45.429284",
     "exception": false,
     "start_time": "2022-06-10T00:07:45.400257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "papermill": {
     "duration": 0.038929,
     "end_time": "2022-06-10T00:07:45.497547",
     "exception": false,
     "start_time": "2022-06-10T00:07:45.458618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dsoftmax(y, out):\n",
    "    dl_dz = out - y\n",
    "    \n",
    "    return dl_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y, out, cache):\n",
    "    inds, word_vec, w, z = cache\n",
    "    wrd_emb, w = params\n",
    "    \n",
    "    dl_dz = dsoftmax(y, out)\n",
    "    # deviding by the word_vec length to find the average\n",
    "    dl_dw = (1/word_vec.shape[1]) * np.dot(dl_dz, word_vec.T)\n",
    "    dl_dword_vec = np.dot(w.T, dl_dz)\n",
    "    \n",
    "    return dl_dz, dl_dw, dl_dword_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, cache, grads, lr=0.03):\n",
    "    inds, word_vec, w, z = cache\n",
    "    wrd_emb, w = params\n",
    "    dl_dz, dl_dw, dl_dword_vec = grads\n",
    "    \n",
    "    wrd_emb[inds.flatten(), :] -= dl_dword_vec.T * lr\n",
    "    w -= dl_dw * lr\n",
    "    \n",
    "    return wrd_emb, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029152,
     "end_time": "2022-06-10T00:07:45.680700",
     "exception": false,
     "start_time": "2022-06-10T00:07:45.651548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "papermill": {
     "duration": 0.53334,
     "end_time": "2022-06-10T00:07:46.311852",
     "exception": false,
     "start_time": "2022-06-10T00:07:45.778512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(id_to_word)\n",
    "\n",
    "m = y.shape[1]\n",
    "y_one_hot = np.zeros((vocab_size, m))\n",
    "y_one_hot[y.flatten(), np.arange(m)] = 1\n",
    "\n",
    "y = y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 3.3387545731034867\n",
      "Cost after epoch 250: 3.324491434962342\n",
      "Cost after epoch 500: 3.1907104986550623\n",
      "Cost after epoch 750: 2.6007879630678463\n",
      "Cost after epoch 1000: 2.195101479381379\n",
      "Cost after epoch 1250: 2.063383359524632\n",
      "Cost after epoch 1500: 2.0151714848607685\n",
      "Cost after epoch 1750: 2.00231486554247\n",
      "Cost after epoch 2000: 1.9944340741314\n",
      "Cost after epoch 2250: 1.9830613630793088\n",
      "Cost after epoch 2500: 1.9719825976193845\n",
      "Cost after epoch 2750: 1.9636453984583393\n",
      "Cost after epoch 3000: 1.957210720130514\n",
      "Cost after epoch 3250: 1.9515369745540534\n",
      "Cost after epoch 3500: 1.9464036370883289\n",
      "Cost after epoch 3750: 1.945096073589045\n",
      "Cost after epoch 4000: 1.9485926019161899\n",
      "Cost after epoch 4250: 1.9536560378262717\n",
      "Cost after epoch 4500: 1.9598958417104668\n",
      "Cost after epoch 4750: 1.9674673852935638\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "embed_size = 50\n",
    "\n",
    "params = init_parameters(vocab_size, 50)\n",
    "\n",
    "costs = []\n",
    "\n",
    "for epoch in range(5000):\n",
    "    epoch_cost = 0\n",
    "    \n",
    "    batch_inds = list(range(0, x.shape[1], batch_size))\n",
    "    np.random.shuffle(batch_inds)\n",
    "    \n",
    "    for i in batch_inds:\n",
    "        x_batch = x[:, i:i+batch_size]\n",
    "        y_batch = y[:, i:i+batch_size]\n",
    "        \n",
    "        pred, cache = forward(x_batch, params)\n",
    "        grads = backward(y_batch, pred, cache)\n",
    "        params = update(params, cache, grads, 0.03)\n",
    "        cost = cross_entropy(pred, y_batch)\n",
    "        \n",
    "        epoch_cost += np.squeeze(cost)\n",
    "        \n",
    "    costs.append(epoch_cost)\n",
    "    \n",
    "    if(epoch % 250 == 0):\n",
    "        print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029768,
     "end_time": "2022-06-10T00:07:46.750715",
     "exception": false,
     "start_time": "2022-06-10T00:07:46.720947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041312,
     "end_time": "2022-06-10T00:07:46.823033",
     "exception": false,
     "start_time": "2022-06-10T00:07:46.781721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = np.arange(vocab_size)\n",
    "x_test = np.expand_dims(x_test, axis=0)\n",
    "softmax_test, _ = forward(x_test, params)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الذكاء's skip-grams: ['التعرف', 'البشري', 'معالجة', 'عادة']\n"
     ]
    }
   ],
   "source": [
    "input_word = 'الذكاء'\n",
    "input_ind = word_to_id[input_word]\n",
    "output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "print(\"{}'s skip-grams: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Modèle GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 61.78592035714478\n",
      "Epoch 10, Loss: 42.33745461131349\n",
      "Epoch 20, Loss: 30.833511790879324\n",
      "Epoch 30, Loss: 23.423527102372766\n",
      "Epoch 40, Loss: 18.3555061088821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Téléchargement du tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Phrase en arabe\n",
    "sentences = \"\"\"الذكاء الاصطناعي هو مجال يهتم بتطوير الأنظمة التي يمكنها أداء المهام التي تتطلب عادةً الذكاء البشري.\n",
    "تتضمن هذه المهام مثل التعرف على الصور، معالجة اللغة الطبيعية، والتنبؤ. يعمل الذكاء الاصطناعي على تحسين الكفاءة وجودة العمل في مختلف الصناعات.\"\"\"\n",
    "\n",
    "# Nettoyage et tokenisation\n",
    "sentences_cleaned = re.sub('[^\\u0621-\\u064A\\s]', ' ', sentences)\n",
    "words = word_tokenize(sentences_cleaned)\n",
    "\n",
    "# Créer le vocabulaire\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# Taille du contexte\n",
    "context_size = 2\n",
    "\n",
    "# Créer les paires de co-occurrence\n",
    "co_occurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for i in range(context_size, len(words) - context_size):\n",
    "    target = words[i]\n",
    "    target_idx = word_to_ix[target]\n",
    "    context = [words[i - j] for j in range(1, context_size + 1)] + [words[i + j] for j in range(1, context_size + 1)]\n",
    "    for ctx_word in context:\n",
    "        context_idx = word_to_ix[ctx_word]\n",
    "        co_occurrence_matrix[target_idx, context_idx] += 1\n",
    "\n",
    "# GloVe - Initialisation des vecteurs\n",
    "embed_dim = 10  # Dimension des vecteurs d'embedding\n",
    "W = np.random.rand(vocab_size, embed_dim)  # Embedding des mots\n",
    "b = np.random.rand(vocab_size)  # Bias pour chaque mot\n",
    "b_context = np.random.rand(vocab_size)  # Bias pour chaque mot de contexte\n",
    "\n",
    "# Fonction de perte pour GloVe\n",
    "def glove_loss(co_occurrence_matrix, W, b, b_context, alpha=0.75, x_max=100, learning_rate=0.01):\n",
    "    loss = 0\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if co_occurrence_matrix[i, j] > 0:\n",
    "                # Poids de l'erreur\n",
    "                weight = (co_occurrence_matrix[i, j] / x_max) ** alpha if co_occurrence_matrix[i, j] < x_max else 1\n",
    "                # Calcul de l'erreur pour cette paire\n",
    "                cost = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j])) ** 2\n",
    "                loss += cost\n",
    "    return loss\n",
    "\n",
    "# Mise à jour des embeddings avec la rétropropagation\n",
    "def update_embeddings(co_occurrence_matrix, W, b, b_context, learning_rate=0.01):\n",
    "    global vocab_size\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if co_occurrence_matrix[i, j] > 0:\n",
    "                # Poids de l'erreur\n",
    "                weight = (co_occurrence_matrix[i, j] / 100) ** 0.75 if co_occurrence_matrix[i, j] < 100 else 1\n",
    "                # Calcul des gradients\n",
    "                gradient_W_i = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j])) * W[j]\n",
    "                gradient_W_j = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j])) * W[i]\n",
    "                gradient_b_i = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j]))\n",
    "                gradient_b_context_j = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j]))\n",
    "\n",
    "                # Mise à jour des embeddings\n",
    "                W[i] -= learning_rate * gradient_W_i\n",
    "                W[j] -= learning_rate * gradient_W_j\n",
    "                b[i] -= learning_rate * gradient_b_i\n",
    "                b_context[j] -= learning_rate * gradient_b_context_j\n",
    "    return W, b, b_context\n",
    "\n",
    "# Entraînement du modèle GloVe\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    loss = glove_loss(co_occurrence_matrix, W, b, b_context)\n",
    "    W, b, b_context = update_embeddings(co_occurrence_matrix, W, b, b_context)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de similarité entre 'الذكاء' et 'الاصطناعي':\n",
      "Similarité cosinus entre 'الذكاء' et 'الاصطناعي': 0.3590775039820202\n",
      "\n",
      "Test d'analogie 'الذكاء' - 'الاصطناعي' + 'يعمل':\n",
      "Les mots les plus similaires à l'analogie 'الذكاء - الاصطناعي + يعمل':\n",
      "Mot: والتنبؤ, Similarité: 0.7383184306079075\n",
      "Mot: الكفاءة, Similarité: 0.7197392514055865\n",
      "Mot: الذكاء, Similarité: 0.7050008422013012\n",
      "Mot: هذه, Similarité: 0.6789067174990286\n",
      "Mot: معالجة, Similarité: 0.6575608758963057\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Fonction pour calculer la similarité cosinus entre deux vecteurs\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Tester la similarité entre deux mots\n",
    "def test_similarity(word1, word2, word_to_ix, ix_to_word, embeddings):\n",
    "    idx1 = word_to_ix[word1]\n",
    "    idx2 = word_to_ix[word2]\n",
    "    similarity = cosine_similarity(embeddings[idx1], embeddings[idx2])\n",
    "    print(f\"Similarité cosinus entre '{word1}' et '{word2}': {similarity}\")\n",
    "\n",
    "# Tester l'analogie \"roi\" - \"homme\" = \"reine\" - \"femme\"\n",
    "def test_analogy(word1, word2, word3, word_to_ix, ix_to_word, embeddings):\n",
    "    # Embeddings des mots\n",
    "    emb1 = embeddings[word_to_ix[word1]]\n",
    "    emb2 = embeddings[word_to_ix[word2]]\n",
    "    emb3 = embeddings[word_to_ix[word3]]\n",
    "    \n",
    "    # Calcul de la direction de l'analogie\n",
    "    analogy_vector = emb1 - emb2 + emb3\n",
    "\n",
    "    # Trouver le mot le plus proche de la direction de l'analogie\n",
    "    similarities = []\n",
    "    for i in range(len(word_to_ix)):\n",
    "        word = ix_to_word[i]\n",
    "        similarity = cosine_similarity(analogy_vector, embeddings[i])\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Trier les mots en fonction de la similarité\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Afficher les 5 mots les plus similaires\n",
    "    print(f\"Les mots les plus similaires à l'analogie '{word1} - {word2} + {word3}':\")\n",
    "    for word, similarity in similarities[:5]:\n",
    "        print(f\"Mot: {word}, Similarité: {similarity}\")\n",
    "\n",
    "# Phase de test\n",
    "print(\"Test de similarité entre 'الذكاء' et 'الاصطناعي':\")\n",
    "test_similarity('الذكاء', 'الاصطناعي', word_to_ix, ix_to_word, W)\n",
    "\n",
    "print(\"\\nTest d'analogie 'الذكاء' - 'الاصطناعي' + 'يعمل':\")\n",
    "test_analogy('الذكاء', 'الاصطناعي', 'يعمل', word_to_ix, ix_to_word, W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "duration": 9.4663,
   "end_time": "2022-06-10T00:07:47.765671",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-10T00:07:38.299371",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
