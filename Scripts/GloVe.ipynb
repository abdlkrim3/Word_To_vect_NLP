{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Modèle GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 61.78592035714478\n",
      "Epoch 10, Loss: 42.33745461131349\n",
      "Epoch 20, Loss: 30.833511790879324\n",
      "Epoch 30, Loss: 23.423527102372766\n",
      "Epoch 40, Loss: 18.3555061088821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Téléchargement du tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Phrase en arabe\n",
    "sentences = \"\"\"الذكاء الاصطناعي هو مجال يهتم بتطوير الأنظمة التي يمكنها أداء المهام التي تتطلب عادةً الذكاء البشري.\n",
    "تتضمن هذه المهام مثل التعرف على الصور، معالجة اللغة الطبيعية، والتنبؤ. يعمل الذكاء الاصطناعي على تحسين الكفاءة وجودة العمل في مختلف الصناعات.\"\"\"\n",
    "\n",
    "# Nettoyage et tokenisation\n",
    "sentences_cleaned = re.sub('[^\\u0621-\\u064A\\s]', ' ', sentences)\n",
    "words = word_tokenize(sentences_cleaned)\n",
    "\n",
    "# Créer le vocabulaire\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# Taille du contexte\n",
    "context_size = 2\n",
    "\n",
    "# Créer les paires de co-occurrence\n",
    "co_occurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for i in range(context_size, len(words) - context_size):\n",
    "    target = words[i]\n",
    "    target_idx = word_to_ix[target]\n",
    "    context = [words[i - j] for j in range(1, context_size + 1)] + [words[i + j] for j in range(1, context_size + 1)]\n",
    "    for ctx_word in context:\n",
    "        context_idx = word_to_ix[ctx_word]\n",
    "        co_occurrence_matrix[target_idx, context_idx] += 1\n",
    "\n",
    "# GloVe - Initialisation des vecteurs\n",
    "embed_dim = 10  # Dimension des vecteurs d'embedding\n",
    "W = np.random.rand(vocab_size, embed_dim)  # Embedding des mots\n",
    "b = np.random.rand(vocab_size)  # Bias pour chaque mot\n",
    "b_context = np.random.rand(vocab_size)  # Bias pour chaque mot de contexte\n",
    "\n",
    "# Fonction de perte pour GloVe\n",
    "def glove_loss(co_occurrence_matrix, W, b, b_context, alpha=0.75, x_max=100, learning_rate=0.01):\n",
    "    loss = 0\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if co_occurrence_matrix[i, j] > 0:\n",
    "                # Poids de l'erreur\n",
    "                weight = (co_occurrence_matrix[i, j] / x_max) ** alpha if co_occurrence_matrix[i, j] < x_max else 1\n",
    "                # Calcul de l'erreur pour cette paire\n",
    "                cost = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j])) ** 2\n",
    "                loss += cost\n",
    "    return loss\n",
    "\n",
    "# Mise à jour des embeddings avec la rétropropagation\n",
    "def update_embeddings(co_occurrence_matrix, W, b, b_context, learning_rate=0.01):\n",
    "    global vocab_size\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if co_occurrence_matrix[i, j] > 0:\n",
    "                # Poids de l'erreur\n",
    "                weight = (co_occurrence_matrix[i, j] / 100) ** 0.75 if co_occurrence_matrix[i, j] < 100 else 1\n",
    "                # Calcul des gradients\n",
    "                gradient_W_i = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j])) * W[j]\n",
    "                gradient_W_j = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j])) * W[i]\n",
    "                gradient_b_i = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j]))\n",
    "                gradient_b_context_j = weight * (np.dot(W[i], W[j]) + b[i] + b_context[j] - np.log(co_occurrence_matrix[i, j]))\n",
    "\n",
    "                # Mise à jour des embeddings\n",
    "                W[i] -= learning_rate * gradient_W_i\n",
    "                W[j] -= learning_rate * gradient_W_j\n",
    "                b[i] -= learning_rate * gradient_b_i\n",
    "                b_context[j] -= learning_rate * gradient_b_context_j\n",
    "    return W, b, b_context\n",
    "\n",
    "# Entraînement du modèle GloVe\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    loss = glove_loss(co_occurrence_matrix, W, b, b_context)\n",
    "    W, b, b_context = update_embeddings(co_occurrence_matrix, W, b, b_context)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de similarité entre 'الذكاء' et 'الاصطناعي':\n",
      "Similarité cosinus entre 'الذكاء' et 'الاصطناعي': 0.3590775039820202\n",
      "\n",
      "Test d'analogie 'الذكاء' - 'الاصطناعي' + 'يعمل':\n",
      "Les mots les plus similaires à l'analogie 'الذكاء - الاصطناعي + يعمل':\n",
      "Mot: والتنبؤ, Similarité: 0.7383184306079075\n",
      "Mot: الكفاءة, Similarité: 0.7197392514055865\n",
      "Mot: الذكاء, Similarité: 0.7050008422013012\n",
      "Mot: هذه, Similarité: 0.6789067174990286\n",
      "Mot: معالجة, Similarité: 0.6575608758963057\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Fonction pour calculer la similarité cosinus entre deux vecteurs\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Tester la similarité entre deux mots\n",
    "def test_similarity(word1, word2, word_to_ix, ix_to_word, embeddings):\n",
    "    idx1 = word_to_ix[word1]\n",
    "    idx2 = word_to_ix[word2]\n",
    "    similarity = cosine_similarity(embeddings[idx1], embeddings[idx2])\n",
    "    print(f\"Similarité cosinus entre '{word1}' et '{word2}': {similarity}\")\n",
    "\n",
    "# Tester l'analogie \"roi\" - \"homme\" = \"reine\" - \"femme\"\n",
    "def test_analogy(word1, word2, word3, word_to_ix, ix_to_word, embeddings):\n",
    "    # Embeddings des mots\n",
    "    emb1 = embeddings[word_to_ix[word1]]\n",
    "    emb2 = embeddings[word_to_ix[word2]]\n",
    "    emb3 = embeddings[word_to_ix[word3]]\n",
    "    \n",
    "    # Calcul de la direction de l'analogie\n",
    "    analogy_vector = emb1 - emb2 + emb3\n",
    "\n",
    "    # Trouver le mot le plus proche de la direction de l'analogie\n",
    "    similarities = []\n",
    "    for i in range(len(word_to_ix)):\n",
    "        word = ix_to_word[i]\n",
    "        similarity = cosine_similarity(analogy_vector, embeddings[i])\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Trier les mots en fonction de la similarité\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Afficher les 5 mots les plus similaires\n",
    "    print(f\"Les mots les plus similaires à l'analogie '{word1} - {word2} + {word3}':\")\n",
    "    for word, similarity in similarities[:5]:\n",
    "        print(f\"Mot: {word}, Similarité: {similarity}\")\n",
    "\n",
    "# Phase de test\n",
    "print(\"Test de similarité entre 'الذكاء' et 'الاصطناعي':\")\n",
    "test_similarity('الذكاء', 'الاصطناعي', word_to_ix, ix_to_word, W)\n",
    "\n",
    "print(\"\\nTest d'analogie 'الذكاء' - 'الاصطناعي' + 'يعمل':\")\n",
    "test_analogy('الذكاء', 'الاصطناعي', 'يعمل', word_to_ix, ix_to_word, W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "duration": 9.4663,
   "end_time": "2022-06-10T00:07:47.765671",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-10T00:07:38.299371",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
